"""
ASH Penetration Testing Suite - DoS Protection Tests
=====================================================
Tests for Denial of Service protection mechanisms.

Attack Vectors Covered:
- Oversized payload rejection
- Deeply nested JSON rejection/protection
- Excessive scope fields rejection
- Memory exhaustion protection
- Algorithmic complexity attacks
- Slowloris-style attacks on canonicalization
"""

import pytest
import sys
import os
import time
import json

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../packages/ash-python/src'))

from ash.core import (
    ash_canonicalize_json,
    ash_canonicalize_url_encoded,
    ash_build_proof,
    ash_build_proof_hmac,
    ash_build_proof_unified,
    ash_derive_client_secret,
    ash_hash_body,
    ash_generate_nonce,
    ash_generate_context_id,
)
from ash.core.proof import ash_extract_scoped_fields, ash_normalize_scope_fields
from ash.core.errors import CanonicalizationError
from ash.core.types import BuildProofInput


@pytest.mark.security_critical
@pytest.mark.dos_protection
class TestOversizedPayloadRejection:
    """Test oversized payload handling."""

    def test_large_payload_canonicalization_performance(self):
        """
        Attack: Send extremely large payload to slow down processing.
        Defense: Large payloads should be handled within reasonable time.
        """
        # Create large payload (100KB of JSON)
        large_data = {"data": "x" * (100 * 1024)}
        
        start = time.time()
        canon = ash_canonicalize_json(large_data)
        elapsed = time.time() - start
        
        # Should complete in reasonable time (< 1 second for 100KB)
        assert elapsed < 1.0, f"Large payload canonicalization too slow: {elapsed:.2f}s"
        assert len(canon) > 0, "Canonicalization should produce output"

    def test_very_large_payload_performance(self):
        """
        Attack: Send very large payload (1MB+).
        Defense: Should handle or reject gracefully.
        """
        # Create very large payload (1MB)
        huge_data = {"items": [{"id": i, "data": "x" * 100} for i in range(1000)]}
        
        start = time.time()
        try:
            canon = ash_canonicalize_json(huge_data)
            elapsed = time.time() - start
            
            # Should complete within reasonable bounds
            assert elapsed < 5.0, f"Very large payload canonicalization too slow: {elapsed:.2f}s"
        except Exception as e:
            # Some implementations may reject very large payloads
            assert isinstance(e, (CanonicalizationError, MemoryError, RecursionError))

    def test_proof_generation_with_large_payload(self):
        """
        Attack: Use large payload to slow proof generation.
        Defense: Proof generation should handle large payloads efficiently.
        """
        large_payload = ash_canonicalize_json({"data": "x" * 10000})
        
        input_data = BuildProofInput(
            mode="balanced",
            binding="POST /api/test",
            context_id="test-context",
            canonical_payload=large_payload,
        )
        
        start = time.time()
        proof = ash_build_proof(input_data)
        elapsed = time.time() - start
        
        # Proof generation should be fast (just hashing)
        assert elapsed < 0.1, f"Proof generation with large payload too slow: {elapsed:.2f}s"
        assert len(proof) > 0, "Proof should be generated"


@pytest.mark.security_critical
@pytest.mark.dos_protection
class TestDeeplyNestedJsonProtection:
    """Test protection against deeply nested JSON attacks."""

    def test_deeply_nested_object_handling(self):
        """
        Attack: Send deeply nested JSON to cause stack overflow.
        Defense: Should handle deep nesting or reject gracefully.
        """
        # Create deeply nested object (1000 levels)
        data = "value"
        for _ in range(1000):
            data = {"nested": data}
        
        start = time.time()
        try:
            canon = ash_canonicalize_json(data)
            elapsed = time.time() - start
            
            # If it succeeds, should be reasonably fast
            assert elapsed < 5.0, f"Deep nesting processing too slow: {elapsed:.2f}s"
        except RecursionError:
            # RecursionError is acceptable (Python's recursion limit)
            pass

    def test_deeply_nested_array_handling(self):
        """
        Attack: Send deeply nested arrays.
        Defense: Should handle or reject gracefully.
        """
        # Create deeply nested array
        data = ["value"]
        for _ in range(500):
            data = [data]
        
        start = time.time()
        try:
            canon = ash_canonicalize_json(data)
            elapsed = time.time() - start
            assert elapsed < 5.0, f"Deep array nesting too slow: {elapsed:.2f}s"
        except RecursionError:
            pass  # Acceptable

    def test_wide_object_handling(self):
        """
        Attack: Send object with many keys.
        Defense: Should handle large objects efficiently.
        """
        # Object with 10000 keys
        data = {f"key_{i}": f"value_{i}" for i in range(10000)}
        
        start = time.time()
        canon = ash_canonicalize_json(data)
        elapsed = time.time() - start
        
        # Should complete in reasonable time
        assert elapsed < 3.0, f"Wide object canonicalization too slow: {elapsed:.2f}s"
        
        # Keys should be sorted
        assert len(canon) > 0


@pytest.mark.security_critical
@pytest.mark.dos_protection
class TestExcessiveScopeFieldsRejection:
    """Test protection against excessive scope fields."""

    def test_many_scope_fields_performance(self):
        """
        Attack: Send request with excessive scope fields.
        Defense: Should handle many scope fields efficiently.
        """
        payload = {f"field_{i}": f"value_{i}" for i in range(1000)}
        scope = [f"field_{i}" for i in range(1000)]
        
        start = time.time()
        result = ash_extract_scoped_fields(payload, scope)
        elapsed = time.time() - start
        
        # Should complete in reasonable time
        assert elapsed < 1.0, f"Many scope fields processing too slow: {elapsed:.2f}s"
        assert len(result) == 1000, "All scope fields should be extracted"

    def test_scope_field_normalization_performance(self):
        """
        Attack: Send scope with many duplicates and unsorted fields.
        Defense: Normalization should be efficient.
        """
        # Create scope with duplicates and random order
        import random
        scope = [f"field_{random.randint(0, 100)}" for _ in range(10000)]
        
        start = time.time()
        normalized = ash_normalize_scope_fields(scope)
        elapsed = time.time() - start
        
        # Should complete quickly
        assert elapsed < 1.0, f"Scope normalization too slow: {elapsed:.2f}s"
        assert len(normalized) <= 101, "Duplicates should be removed"

    def test_deeply_nested_scope_paths(self):
        """
        Attack: Use deeply nested scope paths.
        Defense: Should handle deep paths efficiently.
        """
        # Create payload with deep nesting
        payload = {}
        current = payload
        for i in range(100):
            current["level"] = {}
            current = current["level"]
        current["value"] = "deep"
        
        # Scope path to deep value
        scope = [".".join(["level"] * 100 + ["value"])]
        
        start = time.time()
        result = ash_extract_scoped_fields(payload, scope)
        elapsed = time.time() - start
        
        # Should complete in reasonable time
        assert elapsed < 1.0, f"Deep scope path extraction too slow: {elapsed:.2f}s"


@pytest.mark.security_critical
@pytest.mark.dos_protection
class TestMemoryExhaustionProtection:
    """Test memory exhaustion attack protection."""

    def test_circular_reference_detection(self):
        """
        Attack: Create circular reference in payload.
        Defense: Should detect and reject or handle gracefully.
        """
        # Create circular reference
        data = {"name": "test"}
        data["self"] = data  # Circular reference
        
        # Should raise an error (RecursionError or ValueError)
        with pytest.raises((RecursionError, ValueError, CanonicalizationError)):
            ash_canonicalize_json(data)

    def test_repeated_large_string_handling(self):
        """
        Attack: Use repeated references to large strings.
        Defense: Should handle efficiently without memory explosion.
        """
        # Python strings are immutable but let's test large string handling
        large_string = "x" * 100000
        data = {"a": large_string, "b": large_string, "c": large_string}
        
        start = time.time()
        canon = ash_canonicalize_json(data)
        elapsed = time.time() - start
        
        # Should complete in reasonable time
        assert elapsed < 2.0, f"Large string handling too slow: {elapsed:.2f}s"
        assert len(canon) > 0


@pytest.mark.security_critical
@pytest.mark.dos_protection
class TestAlgorithmicComplexityAttacks:
    """Test resistance to algorithmic complexity attacks."""

    def test_hash_collision_attack_resistance(self):
        """
        Attack: Try to create hash collisions in proof generation.
        Defense: SHA-256 is collision-resistant.
        """
        # This is more of a conceptual test - SHA-256 is designed to be collision-resistant
        # Generate many proofs and verify uniqueness
        
        nonce = ash_generate_nonce()
        context_id = ash_generate_context_id()
        binding = "POST|/api/test|"
        client_secret = ash_derive_client_secret(nonce, context_id, binding)
        
        proofs = set()
        for i in range(1000):
            body_hash = ash_hash_body(f'{{"id":{i}}}')
            proof = ash_build_proof_hmac(client_secret, "1704067200000", binding, body_hash)
            proofs.add(proof)
        
        # All proofs should be unique (no collisions)
        assert len(proofs) == 1000, f"Hash collision detected! Only {len(proofs)} unique proofs"

    def test_sorting_complexity_attack(self):
        """
        Attack: Try to exploit sorting algorithm with crafted input.
        Defense: Sorting should be O(n log n) regardless of input.
        """
        import random
        
        # Create worst-case scenario for some sorting algorithms
        # Reverse-sorted keys
        data = {f"key_{i:08d}": i for i in range(10000, 0, -1)}
        
        start = time.time()
        canon = ash_canonicalize_json(data)
        elapsed = time.time() - start
        
        # Should still be reasonably fast (Python's Timsort is O(n log n))
        assert elapsed < 3.0, f"Sorting complexity attack successful: {elapsed:.2f}s"

    def test_url_encoded_sorting_attack(self):
        """
        Attack: Exploit URL-encoded sorting with crafted input.
        Defense: Should handle efficiently.
        """
        # Create many parameters that sort in complex ways
        params = []
        for i in range(5000):
            key = f"key_{i % 10:02d}"  # Many duplicates
            value = f"value_{i:05d}"
            params.append(f"{key}={value}")
        
        query_string = "&".join(params)
        
        start = time.time()
        canon = ash_canonicalize_url_encoded(query_string)
        elapsed = time.time() - start
        
        # Should complete in reasonable time
        assert elapsed < 3.0, f"URL-encoded sorting too slow: {elapsed:.2f}s"


@pytest.mark.security_critical
@pytest.mark.dos_protection
class TestSlowlorisStyleAttacks:
    """Test resistance to slow processing attacks."""

    def test_unicode_normalization_performance(self):
        """
        Attack: Send strings requiring complex Unicode normalization.
        Defense: Should normalize efficiently.
        """
        # Strings with combining characters that require NFC normalization
        test_strings = []
        for i in range(1000):
            # String with many combining characters
            s = "e" + "\u0301" * 10  # e with 10 combining acute accents
            test_strings.append(s)
        
        data = {"items": test_strings}
        
        start = time.time()
        canon = ash_canonicalize_json(data)
        elapsed = time.time() - start
        
        # Should complete in reasonable time
        assert elapsed < 3.0, f"Unicode normalization too slow: {elapsed:.2f}s"

    def test_json_escaping_performance(self):
        """
        Attack: Send strings requiring extensive JSON escaping.
        Defense: Should escape efficiently.
        """
        # String with many characters that need escaping
        special_chars = "\n\r\t\\\"\b\f" * 1000
        data = {"content": special_chars}
        
        start = time.time()
        canon = ash_canonicalize_json(data)
        elapsed = time.time() - start
        
        # Should complete quickly (escaping is linear time)
        assert elapsed < 1.0, f"JSON escaping too slow: {elapsed:.2f}s"
        assert "\\n" in canon, "Newlines should be escaped"


@pytest.mark.security_critical
@pytest.mark.dos_protection
class TestResourceExhaustionAttacks:
    """Test resource exhaustion attack prevention."""

    def test_many_small_requests_simulation(self):
        """
        Attack: Rapid small requests.
        Defense: Each request should be processed efficiently.
        """
        nonce = ash_generate_nonce()
        context_id = ash_generate_context_id()
        binding = "POST|/api/test|"
        client_secret = ash_derive_client_secret(nonce, context_id, binding)
        
        start = time.time()
        for i in range(1000):
            body_hash = ash_hash_body(f'{{"id":{i}}}')
            proof = ash_build_proof_hmac(client_secret, "1704067200000", binding, body_hash)
            assert len(proof) == 64  # HMAC-SHA256 is 64 hex chars
        
        elapsed = time.time() - start
        
        # Should handle many requests quickly
        assert elapsed < 5.0, f"Many requests processing too slow: {elapsed:.2f}s"

    def test_scope_hash_collision_resistance(self):
        """
        Attack: Try to create scope hash collisions.
        Defense: SHA-256 scope hashing is collision-resistant.
        """
        from ash.core.proof import ash_join_scope_fields, ash_hash_body
        
        scope_hashes = set()
        for i in range(1000):
            scope = [f"field_{j}_{i}" for j in range(10)]
            scope_str = ash_join_scope_fields(scope)
            scope_hash = ash_hash_body(scope_str)
            scope_hashes.add(scope_hash)
        
        # All scope hashes should be unique
        assert len(scope_hashes) == 1000, "Scope hash collision detected"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
